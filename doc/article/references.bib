@article{GARFINKEL201356,
title = {Digital media triage with bulk data analysis and bulk_extractor},
journal = {Computers & Security},
volume = {32},
pages = {56-72},
year = {2013},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2012.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167404812001472},
author = {Simson L. Garfinkel},
keywords = {Digital forensics, Bulk data analysis, , Optimistic decompression, Windows hibernation files, EnCase, Forensic path, Margin, Parallelized forensic analysis, Stream-based forensics},
abstract = {Bulk data analysis eschews file extraction and analysis, common in forensic practice today, and instead processes data in “bulk,” recognizing and extracting salient details (“features”) of use in the typical digital forensics investigation. This article presents the requirements, design and implementation of the bulk_extractor, a high-performance carving and feature extraction tool that uses bulk data analysis to allow the triage and rapid exploitation of digital media. Bulk data analysis and the bulk_extractor are designed to complement traditional forensic approaches, not replace them. The approach and implementation offer several important advances over today's forensic tools, including optimistic decompression of compressed data, context-based stop-lists, and the use of a “forensic path” to document both the physical location and forensic transformations necessary to reconstruct extracted evidence. The bulk_extractor is a stream-based forensic tool, meaning that it scans the entire media from beginning to end without seeking the disk head, and is fully parallelized, allowing it to work at the maximum I/O capabilities of the underlying hardware (provided that the system has sufficient CPU resources). Although bulk_extractor was developed as a research prototype, it has proved useful in actual police investigations, two of which this article recounts.}
}

@article{garfinkel:american-scientist-digital-forensics,
 author="Simson Garfinkel",
 title="Digital Forensics",
 journal="American Scientist",
 month="Sept--Oct",
 publisher="Sigma Xi",
 year=2013,
 url="http://www.simson.net/clips/academic/2013.AmericanScientist.pdf"
}


@article{CASEY2019100873,
title = {Standardization of file recovery classification and authentication},
journal = {Digital Investigation},
volume = {31},
pages = {100873},
year = {2019},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1742287618304602},
author = {Eoghan Casey and Alex Nelson and Jessica Hyde},
keywords = {Digital forensics, Forensic science, Software development, Tool validation, Tool testing, ISO/IEC 27041, ISO/IEC 17025, File recovery, Taxonomy, Standards, SQLite recovery, CASE, DFXML},
abstract = {Digital forensics can no longer tolerate software that cannot be relied upon to perform specific functions such as file recovery. Indistinct and non-standardized results increase the risk of misinterpretation by digital forensic practitioners, and hinder automated correlation of file recovery results in forensic analysis and tool testing. Treating file recovery results in a clear, distinct manner helps reduce the risk of misunderstandings, incorrect assertions and, ultimately, miscarriages of justice. The root of this problem is a lack of clearly defined software requirements, which compels users and tool testers to make educated guesses and assumptions about how digital forensic tools work. To address this problem, this work applies the core forensic processes of classification, authentication and evaluation to file recovery. Specifically, this work defines a vocabulary for software developers, testers and practitioners to classify, authenticate, evaluate and present results of file recovery operations. This vocabulary can be used by software developers to normalize how file recovery is treated, improving clarity, testability and interoperability of results, and reducing the risk or mistakes in digital investigations. This work also proposes an inaugural set of requirements for applying this vocabulary to file recovery results, providing a foundation for further development by the digital forensic community. This work demonstrates how this vocabulary can be implemented using DFXML, and presents a normalized representation of file recovery results using the Cyber-investigation Analysis Standard Expression (CASE). To demonstrate the more generalized utility of this vocabulary, it is applied to recovery results from versioning file systems and SQLite databases. The formalized vocabulary and forensic methods developed in this work support tool validation as called for in the international standard ISO/IEC 27041 and required for accreditation under the international standard ISO 17025. This work also demonstrates how the European Network of Forensic Science Institutes (ENFSI) Guideline for Evaluative Reporting can be applied to express the results of file recovery classification, authentication and evaluation.}
}
\documentclass[5p]{elsarticle}
\usepackage{lineno,hyperref}
\usepackage{shortvrb} \MakeShortVerb{\|}
\usepackage{xspace}
\newcommand{\be}{\texttt{bulk\_extractor}\xspace}
\modulolinenumbers[5]
\journal{Journal Peer Review Conferences}
\bibliographystyle{model2-names}\biboptions{authoryear}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Cross-reference macros
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1} (p.~\pageref{#1})}
\newcommand{\secnameref}[1]{Section~\ref{#1}, ``\nameref{#1}'' (p.~\pageref{#1})}

\newcommand{\partref}[1]{ Part~\ref{#1}, ``\nameref{#1}'' (p.~\pageref{#1})}
\newcommand{\partrefbold}[1]{\textbf{Part~\ref{#1}, ``\nameref{#1}''} (p.~\pageref{#1})}
\newcommand{\appendixref}[1]{Appendix~\ref{#1}}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{frontmatter}

\title{Sharpening Your Tools: Updating {\tt bulk\_extractor} for the 2020s.}
%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author{Simson Garfinkel}
\address{Arlington, VA}

%% or include affiliations in footnotes:

\begin{abstract}
Years of work...
\end{abstract}

\begin{keyword}
bulk\_extractor
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
Digital forensics is a fast moving field. In general, a digital
investigator must be able to analyze ``any data that might be found on
any device anywhere on the planet.''\cite{article} As a result,
digital forensics tools must be continually updated to address new
file formats, new encoding schemes, and new ways that the subjects of
an investigation (targets) use their computers.

The fact that many digital forensics tools are software that run on
stock operating systems (MacOS, Linux and Windows) adds another layer
of complexity: operating systems are constantly evolving, and the
analysts that do not update their systems risk having their systems
compromised by malware. This is true even for analyst workstations
that are ``air gapped'' and not connected to the Internet. Although
new versions of operating systems attempt to provide
compatiability for software that ran on previous versions, these
compatability layers are not perfect. Perhaps more importantly, such
compatability layers typically emphasize compatability for software
that is distributed as compiled binary executables: applications
distributed in source code frequently requires some updating when
there is a new major release of an operating system, a computer
language, or a library on which the application depends. In theory
this isn't an insurmountable problem, because the user has the source
code and is therefore able to update the application (or pay for a
programmer to update the application). In practice, many users of
digital forensic tools lack both the expertise and funding to update
their applications.

This article presents the updating of the digital forensics tool \be.
In \secref{background} we present \be, including the tool's history,
it's use in digital forensics research and education. We also discuss
why it is difficult to ascertain the extent that an open source digital
forensics tool is used operationally. In \secref{analysis} we present
an analysis of the \be verison 1.6 code base.\footnote{Although \be version 1.5.3
is the most recent version of the program that was widely in use at
the start of this project, v1.5.3 would
no longer compile on modern systems. Therefore we had to separately
update v1.5.3, producing version 1.6, in addition to our rewrite that
produced v2.0.}

In \secref{improvement} we present how we planned the update of the
tool. Included in this section is a discussion of the improvements in
the C++ and Python programming languages over the past decade that
might inspire others to upgrade their tools.

\section{Background}\label{background}

Broadly there are two strategies for trying to extract useful
information from a block of data. The first is to use some kind of metadata or external
information to decode the data. The second is to introspectively look
at the data and attempt to extract any information in a recognizable
format. Most digital forensics employ a hybride of these two
strategies.

For example, The Sleuth Kit's |mmls| tool will attempt to identify the
partition type, and |fls| will attempt to identify the file system
type, but |mmls| will not identify a file system and hand the data off
to |fls| for analysis. Likewise, a JPEG carving tool such as photorec
will look for JPEGs, but it will not proactively scan the disk being
carved for GZIP files with the hope that there might be a
GZIP-compressed TAR file that contains JPEGs.

The \be tool is a different kind of digital forensics tool. It bases
its analysis entirely on the contents of the data being analyzed,
proactively attempting to decode every byte of data with every tool in
its arsenal. To recast the above examples, \be will examine every
input block to see if that block contains FAT32 or NTFS directory
entries and, if any are found, they are reported in the
output. Likewise, it scans every byte see if that byte is the
beginning of a JPEG and, if it is, the JPEG is carved. And every byte
is examined to see if it is the beginning of a GZIP-compressed stream
and, if so, the program attempts to decompress that stream and then
recurisvely re-analyzes it for JPEGs, directory entries, and other
kinds of information that might possibly be stored within a compressed
stream.

\subsection{History}
\subsection{Use in research and education}
\subsection{Estimating use in the field}

\section{Analyzing \be}\label{analysis}
\section{Planning the Upgrade}\label{improvement}
\subsection{Upgrade Goals}

\paragraph{Make the program easier to compile and maintain by relying on the
C++ standard.} This was the primary reason for the \be upgrade: the
program no longer compiled on modern operating systems. In part, this
was because \be pre-dated the C++11 standard. As a result, the program
shipped with a complex set of files for \emph{GNU autoconf}
file. Although the autoconf system is resillient, it has its limits,
and after six years of abaondment, they were beginning to show.

We were especially eager to rely on the C++ standard to provide platform-independence, because the
C++ standard is designed so that conforming code can compile in the
future on platforms that do not exist today. It does this by
specifying the \emph{version} of the standard to use when compiling
and linking the executable: C++11, C++14, C++17 and so-on. Using the
C++ standard meant:

\begin{itemize}

\item Using C++ functionality rather than POSIX or Windows functionality.

\item Choosing a specific C++ standard that we used, and then specifying that standard to the compiler.

\item Removing as many |#ifdef| preprocessor directives as possible.

\item Replacing code that the original authors had painstakingly written, debugged and
maintained with code now supported in the C++ standard. This meant
that we might be introducing bugs into working code, so we needed to
have a better strangey for unit testing than the original authors.

\end{itemize}

We originally choose the C++14 standard, as C++17 was not widely
available when this project started. We eventually migrated to C++17 to get the
|std::filesystem| support, and because the project had dragged on for
so long.

\paragraph{Improve reliability}

\paragraph{Make it easier for others to contribute.}

\paragraph{Remove experimental code from the code base}

\paragraph{simplifiy the code base.}

\paragraph{Make the program faster}

Our final objective was to improve the performance of the program.


\subsection{Addressing \be's limitations}
\subsection{Improvements in C++}
\subsection{Improvements in Python}
\subsection{Improvements in testing}
We decided to track and systematically increase the \emph{code
coverage} of the unit tests. As noted above, previously \be had no
unit tests. We started by picking a unit test framework. After
reviewing several available for C++, we chose Catch2 becuase it was
header-only (which meant that it was easier to integrate) and it
appeared to be well supported and well maintained.

We started with unit tests for the BE13 framework. We wrote the unit
tests at the same time we did the refactoring. We generally wrote unit
tests as the new interfaces were designed and implemented. Legacy
interfaces generally did not get unit tests.  We used the popular
CodeCov.io website to display the code coverage of the unit tests.

Creating code coverage reports for C++ was straightfowrard: we re-ran
configure specifying additional compiler flags and libraries, then run
a post-processing tool after the unit test runs, and finally run
CodeCov's script to upload the report to the website. After we got
this working, we then integrated it with GitHub's ``Actions,'' so that
the unit tests would automatically be run and coverage reports
uploaded after every commit to GitHub or pull request.

After all of the new and refactored code had unit tests, we examined
the code coverage reports to determine which pieces of legacy code
were not covered by the newly written unit tests. In some cases,
legacy code \emph{was} covered by the new tests, because the new code
called the old code. But for roughly two thrids of the code, there was
no coverage by unit tests. For this legacy code, writing unit tests
seemed largely like a compliance
exercise---after all, \be has been in use for more than a decade, so
we thought that all of the significant bugs, such a memory allocation
errors, off-by-one errors, and so on, were gone from the code base.
This turned out to be the case! However, the act of writing the unit
tests forced us to clarify internal documentation, simplify
internal implementations, and in some cases we were able to eliminate
legacy code that was no longer being used. To paraphrase the immortal
Steve Jobs, the most reliable piece of code, the piece of code that
you never need to test, is the line of code that you don't write---or
in this case, the line of code that you remove from your legacy programs.


\section{Implementing the Upgrade}

Dramatically improve reliability. This means:
* Whereas previously many strings were passed by reference as |const std::string &|,
there are now passed by value |std::string|. This necessitates a
string copy, but it is not a meaningful impact on performance,
especially when compared with the improved safety against possibly
using an invalidated reference.

* we defined a clear allocation/deallocation policy for all
objects in memory, especially the sbuf objects.

* We enabled sbuf child tracking. Previously this was turned off
because of an underlying memory allocation bug. Once we defined the
clear policy described above, we were able to find the bug!

* Unit tests and code-coverage, rather than simply end-to-end
regression tests.

* Moved more functionality into the |sbuf_t| structure, such as
computing the hash of an sbuf. We also now cache the hash, so we are
assured that each block of data will only be hashed once.


Code Quality Goals:
* Significantly reduce the size of the |bulk_extractor| code base.
* Eliminate global variables used to track state. (Global variables
used to implement static tables and data-driven functions were allowed
to persist, although the code that uses these variables now checks to
verify that they have been initialized and throws an exception if they
have not.)
* Removal of return codes that must be checked to detect
errors. Instead, we use the C++ exception mechanism to signal and
catch error conditions.
* Elimination of explicit mutexs when possible, replaced with the C++
atomic template.

Performance Goals:
* Run twice as fast as the original bulk\_extractor when compiled with
the same (modern) compiler with the same number of cores.
* Be able to run on a cluster with a high-performance storage system,
such as Amazon EBS or S3, and process with Amazon Lambda, with the
goal of being able to process a terabyte reference disk image in 5
minutes.

Functionality Goals:
* Link with The Sleuth Kit and automatically process non-contiguous
files if the file system is intact. Automatically report features by
both location on the disk and the file in which they are found. We did
this in a backwards compatiable manner by adding an addition field to
the pos0\_t record.
(requires an additional field be added to the feature file).

* Removing the dependence on Java for the user interface.



Removed functionality:
- TO the best of our knowledge, no one (other than the original
developer) ever used bulk\_extractor's shared library to let the
program's scanner system be called from C++ or from Python, so we did
dropped support for that. (It could be trivially added in the future.)

- The stand-alone bulk\_extractor test program that only scans a single
file was dropped as additional code that did not need to be
maintained. Instead, we now have unit tests.

- The ability to load scanners as shared libraries at startup was
kept, although it is not clear that this has ever been used. It was
created so that bulk\_extractor users could develop and maintain their
own scanners without telling the developers of them. This might be
useful if \be was being used in a highly restricted environment. If
such use was ever made, the developers were never told! Such unknown
users are advised that they will need to revise their scanners for
version 2.0.


- Moved common code out of the scanners and into the framework. For
example, rather than each scanner having options for setting its carve
mode, the framework understands how to set them for any named
scanner.

(explain carve mode.)

Simplified the API. sbuf previously had a two map\_file's - one that
took filename already open and the fd, and ht eother which just took
the filename. Now it just has one; if it is already open, then it is
oepened a secon time.


* Use google searches to see if options are being used. Based on this,
we were able to eliminate the following options:
- [ ] opt\_work\_start\_work\_end - "Record work start and end of each scanner in report.xml file");

- [ ] opt\_enable\_histograms- "Disable generation of histograms"

Wordlist had its own writer.


To do:
Some scanners (such as wordlist) still have global state. There should
be a way to allocate data within the scanner\_set, perhaps using
tmalloc.

- we call get\_scanner a lot. Would be better to store this in local
space for each scanner.
The author names and affiliations could be formatted in two ways:
\begin{enumerate}[(1)]
\item Group the authors per affiliation.
\item Use footnotes to indicate the affiliations.
\end{enumerate}
See the front matter of this document for examples. You are recommended to conform your choice to the journal you are submitting to.


\subsection{Validating}

In comparing the output of be16 and BE20, we discovered bugs in the
BE16 output that had never been validated. Most of these had to do
with the location of recursively-analyzed features in the feature
file.

For example, the BE forensic path |456536-ZIP-1255117| is read to mean
that there is a feature that is located 1255117 bytes into a
inflated ZIP stream that is itself located 456536 bytes from the
beginning of the disk image. With the disk image nps-2010-emails, BE16
reported the ZIP stream beginning at 456536, but BE20 reports the same
ZIP stream beginning at location 456596. The 60-byte difference is the
result of the ZIP header. BE20 correctly reported that the ZIP stream
began at 456596 because the address was tracked automatically by the
revised memory allocation routines that tracked the location of the
sliced buffer that was handed to the decompressor. In BE16, the
address was computed with explicit code, and that explicit code
(rightly or wrongly) computed the location of the ZIP segment header,
rather than the ZLIB-deflated stream.

In this case we concluded that BE16 was actually wrong.

\begin{figure*}
\begin{Verbatim}
nps-2010-emails_be16/email.txt:456536-ZIP-0-MSXML-9	iwork09.comkeynote_comment@iwork09.com	\x0Akeynote@iwork09.comkeynote_comment@iwork09.com\x0A
nps-2010-emails_be16/email.txt:456536-ZIP-1255117	keynote@iwork09.com	nk href="mailto:keynote@iwork09.com?subject="><sf:s
nps-2010-emails_be16/email.txt:456536-ZIP-1255189	keynote@iwork09.com	racterStyle-38">keynote@iwork09.com</sf:span></sf:l
nps-2010-emails_be20v1/email.txt:456596-ZIP-0-MSXML-9	iwork09.comkeynote_comment@iwork09.com	\x0Akeynote@iwork09.comkeynote_comment@iwork09.com\x0A
nps-2010-emails_be20v1/email.txt:456596-ZIP-1255117	keynote@iwork09.com	nk href="mailto:keynote@iwork09.com?subject="><sf:s
nps-2010-emails_be20v1/email.txt:456596-ZIP-1255189	keynote@iwork09.com	racterStyle-38">keynote@iwork09.com</sf:span></sf:l
\end{Verbatim}
\end{figure*}

Many of hte code paths in the BE16 code base were painstakingly
developed on specific test cases, but those test cases were not added
to the code base as unit tests. For example, the scan\_net network
packet scanner, used as the basis of the [BEVERLY-GARFINKEL] article,
could carve IPv4 and IPv6 packets as well as recognize in-memory TCP
header structures from Microsoft Windows memory dumps. The part of the
scanners that accessed raw memory received significant rewrites to go
through the new sbuf structure. We then wanted to validate that the
rewritten scanners had the same functionality as the old ones. THe
only way to do this was by assembling specific test cases for eaach
data type---and adding them to the code base. This is something that
wasn't done originally,

\subsection{Performance Tuning}
Despite the effort to eliminate all memory copies, the initial verison
of BE2.0 was dramatically slower than BE1.6. For example, scanning the
2009-domexusers disk image on a 6-core macMini required approximately 10 minutes
with BE1.6, but took 70 minutes with BE2.0.

BE has long had the ability to measure the contribution to runtime
caused by each scanner. Specifically, it keeps counters (in
std::atomic variables) of how many times each scanner is called and
how many nanoseconds it spends executing. These counters became more
accurate in BE2.0, with the decision to queue the recursive processing
of sbufs longer than 4K to another thread. Looking at these counters
we saw that just three scanners (rar, net, and aes) were responsible
for the vast majority of the time spent scanning.

Each of these scanners has a hand-coded loop that scans through the
memory image looking for a magic number. The loop had been implemented
making a new sbuf for each location. The subfs weren't copied, but
they were created.  Analysis of a 2GiB disk image required creating
over 3 billion sbufs!  The first improvement we made was to implement
the validator so that instead of validating the first position in the
sbuf, it would take an offset. This eliminated the need to create a
new sbuf. Once the magic number was found, a new sbuf was created, so
as to take advantage of the algorithmic simplification. Additional
improvements were realized by moving the search for magic numbers into
the sbuf implementation itself, so that it could be performed with
memchr. (We hope to move this to the boyer-meyer search implementation
e.g. https://titanwolf.org/Network/Articles/Article?AID=dd71b04e-fe50-4be5-9908-067f4af9afb5#gsc.tab=0)


upgrades:
- converts utf-16 features to utf-8 for the feature file in the
'feature' column, leaves as-is in the context column.

changed the feature file from hex-escaping to octal. This is because
the C++ langauge defines \verb+"\001ab"+ as two-character string of
with a character 001 followed by the character 'a' and then the
character 'b'. However, C++ defines \verb+"\x01ab"+ as a 16-byte value
0x01ab, or a 2-character string. This is confusing. Handling of octal
constants is unambigious.


Things to try:
* show impact of adding cores on a machine with 96 cores, which keeps
the I/O system constant.  use

* re-run on the original laptop, comparing with BE1.6

* Show impact of compiling with no-opt and -O1, -O2 and -O3

\section*{References}

\bibliography{references}

\end{document}
